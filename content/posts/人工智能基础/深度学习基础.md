## 感知机（Perception）

![image-20251230003425039](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230003425039.png)

## 激活函数

![image-20251230003538726](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230003538726.png)

## 人工神经网络

![image-20251230004055761](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230004055761.png)

![image-20251230005226264](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230005226264.png)

## 深度学习

> 特指基于深层神经网络模型和方法的机器学习方法。

**深度学习快速发展的条件：**

- GPU的发展，带来建模算力的提升
- 大数据带来海量的训练数据
- ReLU激活函数被提出，可以有效缓解梯度消失问题

## 自编码器

![image-20251230074210653](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230074210653.png)

## 卷积神经网络

> Convolutional Neural Network是一种最常用的深度学习网络，它主要用于对空间上有相关性的数据进行建模。通过卷积与池化操作，显著减小参数量，适用于具有空间关系的数据。

![image-20251230074432637](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230074432637.png)

![image-20251230074825109](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230074825109.png)

## 循环神经网络（Recurrent Neural Network）

> 主要用于对时间上有相关性的数据进行建模

![image-20251230075208403](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230075208403.png)

###  Long Short-Term Memory

![image-20251230075549697](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230075549697.png)

### 其他

![image-20251230075649815](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230075649815.png)

## 深度学习常用的超参数

### 学习率（Learning Rate）

- 学习率的值如果设置过大，模型可能在训练过程中不稳定，导致损失函数无法收敛；
- 如果设置过小，训练则可能非常缓慢，甚至陷入局部最优解。
- 一般来说，可以采用学习率衰减（Learning Rate Decay）的方法，从较大的初始学习率逐渐减小，或者使用自适应优化方法（如Adam、RMSprop），使学习率能够动态调整，从而加快收敛速度并提高稳定性。

### 优化器（Optimizer）

> 优化器（Optimizer）：优化器决定了如何更新模型的权重，从而最小化损失函数。

- 常见的优化器包括SGD（随机梯度下降）、Adam、RMSprop等。
- SGD是最基础的优化方法，但在大多数情况下速度较慢且不易收敛。
- Adam结合了动量和自适应学习率调整，适用于大部分深度学习任务，因其收敛速度快且对超参数设置较为不敏感。

### 批次大小（Batch Size）

> 批次大小（Batch Size）指的是每次参数更新时使用的训练样本数量。

- 较大的批次大小通常可以加速训练，但也需要更多的显存资源；
- 而较小的批次大小则可能导致训练不稳定。
- 通常的设置方法是从一个合理的初始值开始（例如32或64），然后根据训练资源和效果进行调整。在一些应用中，还可以采用动态批次大小的策略，使模型在训练后期使用更大的批次，从而提高收敛速度。

### 正则化

> 正则化（Regularization）指训练过程中对参数的特定约束，通常能够有效提升模型的泛化能力。

- 在深度学习中，常用的正则化技术有L1和L2正则化，它们通过在损失函数中增加惩罚项来防止模型过拟合。
- 此外，Dropout是一种非常常用的正则化方法，它通过在训练过程中随机丢弃部分神经元，来减少模型对特定神经元的依赖，从而提升模型的泛化能力。

### 权重初始化（weight initialization）

> 指网络参数初始设置的数值

- 常用的权重初始化方法有Xavier初始化和He初始化，它们根据网络的激活函数来合理分配初始权重，避免梯度消失或梯度爆炸的问题，从而使模型能够更好地开始训练。

### 训练轮数（epochs）

> 一个epoch是指将整个训练数据集完整地通过神经网络一次的过程，也可以理解为模型对所有训练样本学习一次。

- 通常设定的轮数过少会导致模型欠拟合，效果不佳。
- 而过多的轮数会消耗大量时间，还会导致模型过拟合。
- 通常可以通过设置验证集，选择合适的epochs参数，如模型loss不再降低的轮数。

## 数据增强（data augmentation）

> 数据增强（Data Augmentation）是一种提升深度学习模型训练效果的常用方法，它通过对训练数据进行各种变换来增加数据集的多样性，从而提升模型泛化能力。尤其是当数据量有限时，数据增强可以显著提高
>
> **图像增强举例：**

![image-20251230102124866](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230102124866.png)

**文本增强：**

![image-20251230102214724](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230102214724.png)

## 人工神经网络的兴起

![image-20251230102512730](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230102512730.png)

![image-20251230102545741](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230102545741.png)

![image-20251230102633256](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230102633256.png)

![image-20251230102710762](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230102710762.png)

![image-20251230102833021](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230102833021.png)

![image-20251230103731828](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230103731828.png)

![image-20251230103803808](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230103803808.png)

![image-20251230103925218](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230103925218.png)

![image-20251230104003249](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230104003249.png)

![**image-20251230104045296**](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230104045296.png)

![image-20251230104356074](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230104356074.png)