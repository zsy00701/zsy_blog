---
title: 机器学习基础
date: '2026-01-01T10:46:33.234Z'
excerpt: 模型、特征、损失函数与优化算法
category: 人工智能基础
---
## 机器学习的核心要素

### 模型

$$
ouput \ y=f(x) \ input
$$

- 输入x被称为特征（feature）
- 输出y被称为目标（target）

**举例：**

⚫ 情感分析：输入 𝑥 是一个句子，输出 𝑦 是该句子的情感（如正面或负面）。
⚫ 语言翻译：输入 𝑥 是一个英语句子，输出 𝑦 是翻译成的中文句子。
⚫ 语音识别：输入 𝑥 是录制的声音片段，输出 𝑦 是对应的文字转录。
⚫ 人脸识别：输入 𝑥 是一张人脸照片，输出 𝑦 是此人的身份。

### 建模

**建模：找到输入和输出之间的函数关系，即 ”𝒇()”**

### 关键阶段

**训练、验证、测试和应用**

![image-20251229225118810](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229225118810.png)

### 数据

**训练数据、验证数据、测试数据、应用数据**

**数据集划分：**

一个典型数据集 𝐷 的形式为：𝐷 = {(𝑥1, 𝑦1 ), (𝑥2, 𝑦2 ), … (𝑥𝑛, 𝑦𝑛)}

✓ 训练数据(𝑫𝒕𝒓𝒂𝒊𝒏)：占总数据的 60%~80%，用于训练模型，优化参数。
✓ 验证数据(𝑫𝒗𝒂𝒍𝒊𝒅𝒂𝒕𝒆)：占 10%~20%，用于调参和验证模型性能。
✓ 测试数据(𝑫𝒕𝒆𝒔𝒕)：占 10%~20%，用于最终性能评估。
✓ 应用数据：通常在实际部署时生成，没有固定比例。通常只有输入 𝑥，模型需要基于已学到的规律预测未知的 𝑦。

**数据划分的注意事项：**

– 验证数据与测试数据的独立性。
– 数据量有限情况下的实际操作。
– 应用数据中的挑战。

**数据划分的几种方法：**

- 留出法（hold out）
- 交叉验证（cross-attention）
- 留一法（leave-one-out） （较少数据时用）

![image-20251229230139566](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229230139566.png)

**机器学习视角下的典型数据分析过程：**

- 准备、加载数据和预处理（特征工程）
- 选择模型
- 训练和评估模型
- 调整模型的参数以获得更好的性能

## 机器学习的两大核心任务（分类和回归）

### 线性回归

![image-20251229230614707](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229230614707.png)

**一般可以用最小二乘法求解最优参数（有限制条件）：**

![image-20251229231017879](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229231017879.png)

![image-20251229231727813](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229231727813.png)**本质上：**

> **皮尔逊相关系数 = 去均值后向量的余弦相似度**

$$
r = \cos(\theta)
$$

决定系数 $R^2$ = 你的模型“比只猜平均值好多少”。

## 逻辑回归（logistic regression）

![image-20251229234435446](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229234435446.png)

![image-20251229234840417](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229234840417.png)

![image-20251229235416919](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229235416919.png)

**Loss Function：**

![image-20251229235453368](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251229235453368.png)

**梯度下降：**

![image-20251230000144337](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230000144337.png)

**扩展：为什么分类任务使用交叉熵而不是均方误差？**

一个主要原因是梯度计算：
交叉熵的梯度在误差较大时梯度也较大，更新更快；误差较小时梯度较小，更新更稳定。而均方误差的梯度在误差较大时梯度可能也较小，导致收敛缓慢。

## 参数和超参数

> 参数是模型在训练中学到的变量，而超参数是需要在训练开始前手动设定的选项，两者共同决定了模型的性能和效果。

- 参数是模型在训练过程中学到的“内部数值”，这些数值决定了模型如何根据输入数据 𝑥 生成输出 𝑦。通过优化参数，模型能够从训练数据中提取规律并进行预测。

- 超参数是在训练开始之前设定的外部变量，影响模型的结构和学习方式。与参数不同，超参数不是通过训练数据直接优化的，而是通过实验和经验手动调整。 

### 梯度下降的重要超参数：学习率(learning rate)

>  学习率：控制模型每次调整参数的幅度，可以看做每次前进的“步长”。

### 梯度下降的重要超参数：训练轮数(epoch)

> 训练轮数：应该走多少步停止呢？

## 机器学习的建模过程

![image-20251230002411325](https://raw.githubusercontent.com/zsy00701/typora-images/main/image-20251230002411325.png)

## 过拟合和欠拟合

### 过拟合

现象：模型在训练集上表现优异，而在验证和测试集上表现糟糕；
原因：模型“记住”了训练数据而忽略了真正的规律，从而失去对新数据的预测能力。

**过拟合的原因：**

- 训练样本的局限性
- 数据中的噪声和异常值
- 模型复杂度过高（参数过多）

**如何应对过拟合：**

- 增加数据量
- 降低模型复杂度
- 正则化
- 早停
- 数据清洗
- 交叉验证

### 欠拟合

> 欠拟合：训练集效果差，验证集效果也差

**原因：**

- 模型复杂度不够
- 训练轮次不够

