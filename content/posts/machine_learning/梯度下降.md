---
title: machine_learning-
date: '2025-12-17T02:39:00.587Z'
excerpt: >-
  ## what 更新公式统一是： $$ \theta_{t+1} = \theta_t - \alpha \nabla L(\theta_t) $$ -
  $\theta$：参数（如 $w, b$） - $\alpha$：学习率 - $\nabla L$：损失函数的梯度 ## 三种…
category: machine_learning
---
## what

更新公式统一是：
$$
\theta_{t+1} = \theta_t - \alpha \nabla L(\theta_t)
$$

- $\theta$：参数（如 $w, b$）
- $\alpha$：学习率
- $\nabla L$：损失函数的梯度

## 三种最基础的梯度下降

### Batch Gradient Descent

**做法**

- 每一步：**用全部训练数据**算一次梯度
- 再更新一次参数

 **公式**
$$
\nabla L = \frac{1}{n}\sum_{i=1}^n \nabla L_i
$$

### Stochastic Gradient Descent

**做法**

- 每次只用 **1 个样本** 更新一次参数

 **公式**
$$
\theta \leftarrow \theta - \alpha \nabla L_i
$$

### Mini-Batch Gradient Descent

**做法**

- 每次用一小批数据（比如 32 / 64 / 128）

 **公式**
$$
\nabla L = \frac{1}{|\mathcal{B}|}\sum_{i\in\mathcal{B}} \nabla L_i
$$

## 改进版

### Momentum



### Nesterov Momentum（NAG）



## 自适应梯度下降

### AdaGrad

 

### RMSProp



### Adam
